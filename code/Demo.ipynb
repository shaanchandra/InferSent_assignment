{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Preprocessing NLI data---------\n",
      "549367 instances extracted of train\n",
      "9842 instances extracted of dev\n",
      "9824 instances extracted of test\n",
      "Example: train['s1'][0] =  A person on a horse jumps over a broken down airplane .\n",
      "\n",
      " -----------Building Vocab from the SNLI dataset-----------\n",
      "----Getting Glove word embedding for each word in vocab (non vocab words ignored, ie, <unk> not used)----\n",
      "Found 38957 words with Glove embeddings out of 43479 total words in corpus.\n"
     ]
    }
   ],
   "source": [
    "from data import get_nli, build_vocab\n",
    "\n",
    "nli_path = './data/snli'\n",
    "glove_path = './data/glove/glove.840B.300d.txt'\n",
    "\n",
    "train, dev, test = get_nli(nli_path)\n",
    "vocab, embeddings = build_vocab(train['s1']+train['s2']+test['s1']+test['s2']+dev['s1']+dev['s2'], glove_path)\n",
    "\n",
    "# print(type(FLAGS.prem))\n",
    "\n",
    "def get_batch_from_idx(sent, word_emb, config):\n",
    "   \n",
    "    embedded_sents = np.zeros((len(sent), config['emb_dim']))\n",
    "    \n",
    "    \n",
    "    for i in range(len(sent)):\n",
    "        \n",
    "        if config['model_name'] == 'base':\n",
    "            if sent[i] in word_emb:\n",
    "                #return batch embeddings of dimension (L x B x D)\n",
    "                embedded_sents[i, :] = word_emb[sent[i]]/len(sent)\n",
    "            else: embedded_sents[i, :] = np.zeros((config['emb_dim']))\n",
    "\n",
    "        else:\n",
    "            if sent[i] in word_emb:\n",
    "                embedded_sents[i, :] = word_emb[sent[i]]\n",
    "            else: embedded_sents[i, :] = np.zeros((config['emb_dim']))\n",
    "                \n",
    "    return torch.from_numpy(embedded_sents).float(), len(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "from data import get_nli, build_vocab\n",
    "from models import Classifier, LSTM, biLSTM, LSTM_main\n",
    "from dev_test_evals import model_eval\n",
    "\n",
    "MODEL_NAME_DEFAULT = 'bilstm_pool'\n",
    "\n",
    "\n",
    "s1_DEFAULT = 'Bob is in his room, but because of the thunder and lightning outside, he cannot sleep '\n",
    "s2_DEFAULT = 'It is sunny outside'\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model_name', type = str, default = MODEL_NAME_DEFAULT,\n",
    "                      help='model name: base / lstm / bilstm / bilstm_pool')\n",
    "parser.add_argument('--prem', type = str, default = s1_DEFAULT,\n",
    "                      help='premise')\n",
    "parser.add_argument('--hyp', type = str, default = s2_DEFAULT,\n",
    "                      help='hypothesis')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "base_path = './checkout/base_final.pickle'\n",
    "lstm_path = './checkout/lstm_final.pickle'\n",
    "pool_path = './checkout/pool_final.pickle'\n",
    "bi_path = './checkout/bilstm_final.pickle'\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    #Print Flags\n",
    "    for key, value in vars(FLAGS).items():\n",
    "        print(key + ' : ' + str(value))\n",
    "\n",
    "# main() \n",
    "\n",
    "\n",
    "config = {'model_name' : FLAGS.model_name,\n",
    "         'emb_dim' : 300,\n",
    "         'b_size' : 1,\n",
    "         'fc_dim' : 512,\n",
    "          'lstm_dim': 2048,\n",
    "         'n_classes' : 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Hi I am the   bilstm_pool    model...!!\n",
      "\n",
      " Hhhhmmmmm....lemme think...\n",
      "\n",
      "==========================================================================\n",
      "(encrypted) Model output:     tensor([[-3.5458,  1.3864,  2.1545]])\n",
      "==========================================================================\n",
      "\n",
      "Premise: Bob is in his room, but because of the thunder and lightning outside, he cannot sleep \n",
      "Hypothesis: It is sunny outside\n",
      "\n",
      "Prediction is: contradiction\n"
     ]
    }
   ],
   "source": [
    "s1_embed, s1_len = get_batch_from_idx(FLAGS.prem.split(), embeddings, config)\n",
    "s2_embed, s2_len = get_batch_from_idx(FLAGS.hyp.split(), embeddings, config)\n",
    "\n",
    "\n",
    "print(\"\\n\\n Hi I am the   \" + str(FLAGS.model_name) + \"    model...!!\")\n",
    "print(\"\\n Hhhhmmmmm....lemme think...\\n\")\n",
    "\n",
    "\n",
    "if config['model_name']== 'base':\n",
    "    model = Classifier(config).to(device)\n",
    "    PATH = base_path\n",
    "    model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    u = torch.sum(s1_embed,0).to(device)\n",
    "    v = torch.sum(s1_embed,0).to(device)\n",
    "\n",
    "    \n",
    "    feats = torch.cat((u, v, torch.abs(u- v), u*v), 0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            out = model.forward(feats).to(device)\n",
    "            pred = torch.max(out,0)[1]\n",
    "            \n",
    "    \n",
    "else: \n",
    "    if config['model_name'] == 'lstm':\n",
    "        PATH = lstm_path\n",
    "    elif config['model_name'] == 'bilstm':\n",
    "        PATH = bi_path\n",
    "    elif config['model_name'] == 'bilstm_pool':\n",
    "        PATH = pool_path\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = LSTM_main(config).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "    model = model.to(device)\n",
    "\n",
    "    s1_embed = s1_embed.expand(1,s1_len, -1).transpose(0,1)\n",
    "    s2_embed = s2_embed.expand(1,s2_len, -1).transpose(0,1)\n",
    "    \n",
    "    s1_len = torch.as_tensor(s1_len, dtype=torch.int64).expand(1)\n",
    "    s2_len = torch.as_tensor(s2_len, dtype=torch.int64).expand(1)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "            out = model.forward(((s1_embed, s1_len), (s2_embed, s2_len))).to(device)\n",
    "            pred = torch.max(out[0],0)[1]\n",
    "            \n",
    "            \n",
    "print(\"==========================================================================\")\n",
    "\n",
    "print(\"(encrypted) Model output:    \", str(out))\n",
    "\n",
    "print(\"==========================================================================\")\n",
    "print(\"\\nPremise: \" + FLAGS.prem)\n",
    "print(\"Hypothesis: \" + FLAGS.hyp)\n",
    "if pred == 0:\n",
    "    print(\"\\nPrediction is: entailment\")\n",
    "elif pred == 1:\n",
    "    print(\"\\nPrediction is: neutral\")\n",
    "elif pred == 2:\n",
    "    print(\"\\nPrediction is: contradiction\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
